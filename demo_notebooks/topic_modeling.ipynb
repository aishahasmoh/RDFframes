{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from rdfframes.client.http_client import HttpClientDataFormat, HttpClient\n",
    "from rdfframes.knowledge_graph import KnowledgeGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "graph = KnowledgeGraph(\n",
    "    graph_uri='http://dblp.l3s.de',\n",
    "    prefixes={\n",
    "        \"xsd\": \"http://www.w3.org/2001/XMLSchema#\",\n",
    "        \"swrc\": \"http://swrc.ontoware.org/ontology#\",\n",
    "        \"rdf\": \"http://www.w3.org/1999/02/22-rdf-syntax-ns#\",\n",
    "        \"dc\": \"http://purl.org/dc/elements/1.1/\",\n",
    "        \"dcterm\": \"http://purl.org/dc/terms/\",\n",
    "        \"dblprc\": \"http://dblp.l3s.de/d2r/resource/conferences/\"\n",
    "    })\n",
    "\n",
    "endpoint = 'http://10.161.202.101:8890/sparql/'\n",
    "port = 8890\n",
    "output_format = HttpClientDataFormat.PANDAS_DF\n",
    "client = HttpClient(endpoint_url=endpoint,\n",
    "                    port=port,\n",
    "                    return_format=output_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = graph.entities('swrc:InProceedings', entities_col_name='paper')\\\n",
    "    .expand(src_col_name='paper', predicate_list=[\n",
    "        ('dc:creator', 'author'), ('dcterm:issued', 'date'),\n",
    "        ('swrc:series', 'conference'), ('dc:title', 'title')])\n",
    "dataset = dataset.cache()\n",
    "    \n",
    "authors = dataset.filter({'date':['>= 2000'], 'conference': ['IN (dblprc:vldb, dblprc:sigmod)']})\\\n",
    "    .group_by(['author']).count('paper', 'papers_count')\\\n",
    "    .filter({'papers_count':['>= 20']})\n",
    "\n",
    "titles = dataset.join(authors, 'author').filter({'date': ['>= 2005']}).select_cols(['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparql Query = \n",
      "PREFIX xsd: <http://www.w3.org/2001/XMLSchema#>\n",
      "PREFIX swrc: <http://swrc.ontoware.org/ontology#>\n",
      "PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
      "PREFIX dc: <http://purl.org/dc/elements/1.1/>\n",
      "PREFIX dcterm: <http://purl.org/dc/terms/>\n",
      "PREFIX dblprc: <http://dblp.l3s.de/d2r/resource/conferences/>\n",
      "SELECT ?title \n",
      "FROM <http://dblp.l3s.de>\n",
      "WHERE {\n",
      "\t?paper rdf:type swrc:InProceedings .\n",
      "\t?paper dc:creator ?author .\n",
      "\t?paper dcterm:issued ?date .\n",
      "\t?paper swrc:series ?conference .\n",
      "\t?paper dc:title ?title .\n",
      "\tFILTER (  (year(xsd:dateTime(?date)) >= 2005 ) ) \n",
      "\n",
      "\t\t{\n",
      "\t\tSELECT ?author  (COUNT( ?paper) AS ?papers_count) \n",
      "\t\tWHERE {\n",
      "\t\t\t?paper rdf:type swrc:InProceedings .\n",
      "\t\t\t?paper dc:creator ?author .\n",
      "\t\t\t?paper dcterm:issued ?date .\n",
      "\t\t\t?paper swrc:series ?conference .\n",
      "\t\t\t?paper dc:title ?title .\n",
      "\t\t\tFILTER (  (year(xsd:dateTime(?date)) >= 2000 ) &&  (?conference IN (dblprc:vldb, dblprc:sigmod) ) ) \n",
      "\t\t\t} GROUP BY ?author \n",
      "\t\tHAVING ( ( COUNT( ?paper) >= 20 ) )\n",
      "\t\t\n",
      "\t\t}\n",
      "\t}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Sparql Query = \\n{}\".format(titles.to_sparql()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time of the query preparation 0.0005693435668945312\n"
     ]
    }
   ],
   "source": [
    "df = titles.execute(client, return_format=output_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['title'], dtype='object')\n",
      "(4684, 1)\n",
      "                                               title\n",
      "0  A framework for using reference ontologies as ...\n",
      "1  Regular Paths in SparQL: Querying the NCI Thes...\n",
      "2  Automatic XQuery Generation and Generalized Vi...\n",
      "3  Generating Application Ontologies from Referen...\n",
      "4  Laziness is a Virtue: Motion Stitching Using E...\n",
      "5  FMDistance: A Fast and Effective Distance Func...\n",
      "6  Scalable modeling of real graphs using Kroneck...\n",
      "7  Probabilistic Tensor Analysis with Akaike and ...\n",
      "8  SLL: Running My Web Services on Your WS Platfo...\n",
      "9  Opinion Fraud Detection in Online Reviews by N...\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)\n",
    "print(df.shape)\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               title  \\\n",
      "0  A framework for using reference ontologies as ...   \n",
      "1  Regular Paths in SparQL: Querying the NCI Thes...   \n",
      "2  Automatic XQuery Generation and Generalized Vi...   \n",
      "3  Generating Application Ontologies from Referen...   \n",
      "4  Laziness is a Virtue: Motion Stitching Using E...   \n",
      "\n",
      "                                         clean_title  \n",
      "0  framework using reference ontologies foundatio...  \n",
      "1            regular paths sparql querying thesaurus  \n",
      "2  automatic xquery generation generalized visual...  \n",
      "3  generating application ontologies from referen...  \n",
      "4  laziness virtue motion stitching using effort ...  \n"
     ]
    }
   ],
   "source": [
    "# removing everything except alphabets`\n",
    "df['clean_title'] = df['title'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
    "# removing short words\n",
    "df['clean_title'] = df['clean_title'].apply(lambda x: ' '.join([w for w in str(x).split() if len(w)>3])) \n",
    "# make all text lowercase\n",
    "df['clean_title'] = df['clean_title'].apply(lambda x: x.lower())\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/amohamed/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               title  \\\n",
      "0  A framework for using reference ontologies as ...   \n",
      "1  Regular Paths in SparQL: Querying the NCI Thes...   \n",
      "2  Automatic XQuery Generation and Generalized Vi...   \n",
      "3  Generating Application Ontologies from Referen...   \n",
      "4  Laziness is a Virtue: Motion Stitching Using E...   \n",
      "\n",
      "                                         clean_title  \n",
      "0  framework using reference ontologies foundatio...  \n",
      "1            regular paths sparql querying thesaurus  \n",
      "2  automatic xquery generation generalized visual...  \n",
      "3  generating application ontologies reference on...  \n",
      "4  laziness virtue motion stitching using effort ...  \n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Using the stopwords.\n",
    "from nltk.corpus import stopwords\n",
    "# Initialize the stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words = [x.strip() for x in stop_words] + ['based']\n",
    "\n",
    "# tokenization\n",
    "tokenized_doc = df['clean_title'].apply(lambda x: x.split())\n",
    "# remove stop-words\n",
    "tokenized_doc = tokenized_doc.apply(lambda x: [item for item in x if item not in stop_words])\n",
    "# de-tokenization\n",
    "detokenized_doc = []\n",
    "for i in range(len(df)):\n",
    "    t = ' '.join(tokenized_doc[i])\n",
    "    detokenized_doc.append(t)\n",
    "\n",
    "df['clean_title'] = detokenized_doc\n",
    "\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english', \n",
    "max_features= 1000, # keep top 1000 terms \n",
    "max_df = 0.5, \n",
    "smooth_idf=True)\n",
    "\n",
    "X = vectorizer.fit_transform(df['clean_title']) # document-term matrix\n",
    "\n",
    "# SVD represent documents and terms in vectors \n",
    "svd_model = TruncatedSVD(n_components=20, algorithm='randomized', n_iter=100, random_state=122)\n",
    "\n",
    "svd_model.fit(X)\n",
    "\n",
    "len(svd_model.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: data mining query processing management streams networks \n",
      "Topic 1: data management streams quality integration cleaning structured \n",
      "Topic 2: query processing queries stream continuous optimization distributed \n",
      "Topic 3: mining search graphs large graph databases scale \n",
      "Topic 4: search efficient databases keyword similarity database queries \n",
      "Topic 5: queries database management systems using large answering \n",
      "Topic 6: database management systems information query search applications \n",
      "Topic 7: large graphs scale search efficient processing social \n",
      "Topic 8: databases large probabilistic graphs information scale uncertain \n",
      "Topic 9: graph information detection processing approach extraction using \n",
      "Topic 10: information efficient extraction mining queries management graphs \n",
      "Topic 11: efficient time databases computation distributed series detection \n",
      "Topic 12: management processing distributed stream mining queries systems \n",
      "Topic 13: time series databases real distributed detection systems \n",
      "Topic 14: streams privacy detection distributed database preserving analysis \n",
      "Topic 15: privacy preserving using social preservation publishing time \n",
      "Topic 16: management detection query streams using systems distributed \n",
      "Topic 17: using social processing stream detection event databases \n",
      "Topic 18: multi using dimensional peer high systems clustering \n",
      "Topic 19: using networks distributed streams sensor databases privacy \n"
     ]
    }
   ],
   "source": [
    "terms = vectorizer.get_feature_names()\n",
    "\n",
    "for i, comp in enumerate(svd_model.components_):\n",
    "    terms_comp = zip(terms, comp)\n",
    "    sorted_terms = sorted(terms_comp, key= lambda x:x[1], reverse=True)[:7]\n",
    "    string = \"Topic \"+str(i)+\": \"\n",
    "    for t in sorted_terms:\n",
    "        string += t[0] + \" \"\n",
    "    print(string)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rdfframes",
   "language": "python",
   "name": "rdfframes"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
