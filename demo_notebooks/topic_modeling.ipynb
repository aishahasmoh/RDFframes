{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling on DBLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdfframes.client.http_client import HttpClientDataFormat, HttpClient\n",
    "from rdfframes.knowledge_graph import KnowledgeGraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose the graph and define the SPARQL endpoint URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = KnowledgeGraph(\n",
    "    graph_uri='http://dblp.l3s.de',\n",
    "    prefixes={\n",
    "        \"xsd\": \"http://www.w3.org/2001/XMLSchema#\",\n",
    "        \"swrc\": \"http://swrc.ontoware.org/ontology#\",\n",
    "        \"rdf\": \"http://www.w3.org/1999/02/22-rdf-syntax-ns#\",\n",
    "        \"dc\": \"http://purl.org/dc/elements/1.1/\",\n",
    "        \"dcterm\": \"http://purl.org/dc/terms/\",\n",
    "        \"dblprc\": \"http://dblp.l3s.de/d2r/resource/conferences/\"\n",
    "    })\n",
    "\n",
    "endpoint = 'http://10.161.202.101:8890/sparql/'\n",
    "port = 8890\n",
    "output_format = HttpClientDataFormat.PANDAS_DF\n",
    "client = HttpClient(endpoint_url=endpoint,\n",
    "                    port=port,\n",
    "                    return_format=output_format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a dataframe of papers titles from the graph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = graph.entities('swrc:InProceedings', entities_col_name='paper')\\\n",
    "    .expand(src_col_name='paper', predicate_list=[\n",
    "        ('dc:creator', 'author'), ('dcterm:issued', 'date'),\n",
    "        ('swrc:series', 'conference'), ('dc:title', 'title')])\n",
    "dataset = dataset.cache()\n",
    "    \n",
    "authors = dataset.filter({'date':['>= 2000'], 'conference': ['IN (dblprc:vldb, dblprc:sigmod)']})\\\n",
    "    .group_by(['author']).count('paper', 'papers_count')\\\n",
    "    .filter({'papers_count':['>= 20']})\n",
    "\n",
    "titles = dataset.join(authors, 'author').filter({'date': ['>= 2010']}).select_cols(['title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute RDFframes code to get the result in a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               title\n",
      "0  Opinion Fraud Detection in Online Reviews by N...\n",
      "1  WindMine: Fast and Effective Mining of Web-cli...\n",
      "2           Query Log Attack on Encrypted Databases.\n",
      "3     TPC-BiH: A Benchmark for Bitemporal Databases.\n",
      "4  Recommending People in Developers' Collaborati...\n",
      "5  Discovering Subsumption Relationships for Web-...\n",
      "6  Location based Social Network analysis using T...\n",
      "7      Cost and Quality Trade-Offs in Crowdsourcing.\n",
      "8  Randomly Partitioned Encryption for Cloud Data...\n",
      "9                        Structured Data on the Web.\n"
     ]
    }
   ],
   "source": [
    "df = titles.execute(client, return_format=output_format)\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               title  \\\n",
      "0  Opinion Fraud Detection in Online Reviews by N...   \n",
      "1  WindMine: Fast and Effective Mining of Web-cli...   \n",
      "2           Query Log Attack on Encrypted Databases.   \n",
      "3     TPC-BiH: A Benchmark for Bitemporal Databases.   \n",
      "4  Recommending People in Developers' Collaborati...   \n",
      "\n",
      "                                         clean_title  \n",
      "0  opinion fraud detection online reviews network...  \n",
      "1     windmine fast effective mining click sequences  \n",
      "2                   query attack encrypted databases  \n",
      "3                     benchmark bitemporal databases  \n",
      "4  recommending people developers collaboration n...  \n"
     ]
    }
   ],
   "source": [
    "# removing everything except alphabets`\n",
    "df['clean_title'] = df['title'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
    "# removing short words\n",
    "df['clean_title'] = df['clean_title'].apply(lambda x: ' '.join([w for w in str(x).split() if len(w)>3])) \n",
    "# make all text lowercase\n",
    "df['clean_title'] = df['clean_title'].apply(lambda x: x.lower())\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/amohamed/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               title  \\\n",
      "0  Opinion Fraud Detection in Online Reviews by N...   \n",
      "1  WindMine: Fast and Effective Mining of Web-cli...   \n",
      "2           Query Log Attack on Encrypted Databases.   \n",
      "3     TPC-BiH: A Benchmark for Bitemporal Databases.   \n",
      "4  Recommending People in Developers' Collaborati...   \n",
      "\n",
      "                                         clean_title  \n",
      "0  opinion fraud detection online reviews network...  \n",
      "1     windmine fast effective mining click sequences  \n",
      "2                   query attack encrypted databases  \n",
      "3                     benchmark bitemporal databases  \n",
      "4  recommending people developers collaboration n...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Using the stopwords.\n",
    "from nltk.corpus import stopwords\n",
    "# Initialize the stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words = [x.strip() for x in stop_words] + ['based']\n",
    "\n",
    "# tokenization\n",
    "tokenized_doc = df['clean_title'].apply(lambda x: x.split())\n",
    "# remove stop-words\n",
    "tokenized_doc = tokenized_doc.apply(lambda x: [item for item in x if item not in stop_words])\n",
    "# de-tokenization\n",
    "detokenized_doc = []\n",
    "for i in range(len(df)):\n",
    "    t = ' '.join(tokenized_doc[i])\n",
    "    detokenized_doc.append(t)\n",
    "\n",
    "df['clean_title'] = detokenized_doc\n",
    "\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TruncatedSVD(algorithm='randomized', n_components=20, n_iter=100,\n",
       "             random_state=122, tol=0.0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english', \n",
    "max_features= 1000, # keep top 1000 terms \n",
    "max_df = 0.5, \n",
    "smooth_idf=True)\n",
    "\n",
    "X = vectorizer.fit_transform(df['clean_title']) # document-term matrix\n",
    "\n",
    "# SVD represent documents and terms in vectors \n",
    "svd_model = TruncatedSVD(n_components=20, algorithm='randomized', n_iter=100, random_state=122)\n",
    "\n",
    "svd_model.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: data mining networks management query social processing \n",
      "Topic 1: networks social information heterogeneous large mining graph \n",
      "Topic 2: query processing graph large graphs efficient mining \n",
      "Topic 3: query processing database social networks queries memory \n",
      "Topic 4: mining query crowd search frequent processing exploration \n",
      "Topic 5: large graphs social query search scale processing \n",
      "Topic 6: search database efficient keyword memory time social \n",
      "Topic 7: database mining social memory systems analytics service \n",
      "Topic 8: graph social event pattern analytics online streams \n",
      "Topic 9: queries using answering knowledge time event network \n",
      "Topic 10: time analytics real series information processing event \n",
      "Topic 11: databases probabilistic detection knowledge analysis time network \n",
      "Topic 12: search knowledge using extraction analysis social entity \n",
      "Topic 13: detection community event efficient memory streams aware \n",
      "Topic 14: multi learning classification feature detection label selection \n",
      "Topic 15: analysis efficient using memory processing distributed learning \n",
      "Topic 16: learning analytics processing distributed queries knowledge scalable \n",
      "Topic 17: knowledge privacy entity efficient query management dynamic \n",
      "Topic 18: analysis management cloud queries search network multi \n",
      "Topic 19: management cloud using scalable graphs multi time \n"
     ]
    }
   ],
   "source": [
    "terms = vectorizer.get_feature_names()\n",
    "\n",
    "for i, comp in enumerate(svd_model.components_):\n",
    "    terms_comp = zip(terms, comp)\n",
    "    sorted_terms = sorted(terms_comp, key= lambda x:x[1], reverse=True)[:7]\n",
    "    string = \"Topic \"+str(i)+\": \"\n",
    "    for t in sorted_terms:\n",
    "        string += t[0] + \" \"\n",
    "    print(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rdfframes",
   "language": "python",
   "name": "rdfframes"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
